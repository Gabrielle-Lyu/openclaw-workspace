#!/usr/bin/env python3
import argparse
import datetime
import os
import re
import subprocess
import sys
from urllib.parse import urlparse

import requests
from readability import Document

DEFAULT_OUT_DIR = "/home/ubuntu/openclaw-workspace/00 - Inbox/Docs/dropzone"


def slug(s: str) -> str:
    s = s.lower()
    s = re.sub(r"[^a-z0-9]+", "-", s)
    return s.strip("-")[:80] or "doc"


def fetch(url: str) -> str:
    r = requests.get(url, timeout=25, headers={"User-Agent": "openclaw-crawler"})
    r.raise_for_status()
    return r.text


def strip_boilerplate_html(html: str) -> str:
    """Strip common boilerplate/wrapper tags before conversion."""
    # Remove script/style blocks entirely
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    
    # Remove empty wrapper divs/spans (common UI chrome)
    html = re.sub(r'<div[^>]*>\s*</div>', '', html, flags=re.IGNORECASE)
    html = re.sub(r'<span[^>]*>\s*</span>', '', html, flags=re.IGNORECASE)
    
    # Strip attributes from remaining tags (keep tag structure for pandoc)
    html = re.sub(r'<(div|span|p|ul|ol|li|h[1-6]|table|tr|td|th|a|strong|em|code|pre)\s+[^>]*>', r'<\1>', html, flags=re.IGNORECASE)
    
    return html


def clean_markdown(md: str) -> str:
    """Post-process markdown to remove residual HTML."""
    # Remove any remaining HTML tags
    md = re.sub(r'</?div[^>]*>', '', md, flags=re.IGNORECASE)
    md = re.sub(r'</?span[^>]*>', '', md, flags=re.IGNORECASE)
    md = re.sub(r'</?nav[^>]*>', '', md, flags=re.IGNORECASE)
    md = re.sub(r'</?header[^>]*>', '', md, flags=re.IGNORECASE)
    md = re.sub(r'</?footer[^>]*>', '', md, flags=re.IGNORECASE)
    
    # Remove excessive blank lines
    md = re.sub(r'\n{3,}', '\n\n', md)
    
    return md.strip()


def html_to_md(clean_html: str) -> str:
    # Pre-clean HTML
    clean_html = strip_boilerplate_html(clean_html)
    
    p = subprocess.run(
        ["pandoc", "-f", "html", "-t", "gfm", "--wrap=none"],
        input=clean_html.encode(),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    if p.returncode != 0:
        raise RuntimeError(f"pandoc failed: {p.stderr.decode(errors='ignore')}")
    
    md = p.stdout.decode(errors="ignore")
    
    # Post-clean markdown
    md = clean_markdown(md)
    
    return md


def crawl_one(url: str, out_dir: str, filename: str | None = None) -> str:
    html = fetch(url)
    doc = Document(html)

    title = doc.short_title() or "untitled"
    clean = doc.summary()
    md = html_to_md(clean)

    ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    host = (urlparse(url).netloc or "unknown-host").replace(".", "-")

    if filename:
        out_name = filename if filename.endswith(".md") else f"{filename}.md"
    else:
        out_name = f"{ts}__{host}__{slug(title)}.md"

    path = os.path.join(out_dir, out_name)

    front = f"""---
title: \"{title}\"
source: \"{url}\"
fetched: \"{ts}\"
---

"""

    with open(path, "w", encoding="utf-8") as f:
        f.write(front + md)

    return path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Crawl one or many webpages into markdown files."
    )
    parser.add_argument("urls", nargs="+", help="One or more URLs separated by spaces")
    parser.add_argument("--out-dir", default=DEFAULT_OUT_DIR, help="Output directory")
    parser.add_argument(
        "--filename",
        default=None,
        help="Optional output filename (only valid when a single URL is provided)",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    urls = args.urls
    out_dir = args.out_dir
    filename = args.filename

    if filename and len(urls) > 1:
        print("error: --filename can only be used with a single URL", file=sys.stderr)
        return 2

    os.makedirs(out_dir, exist_ok=True)

    ok_paths = []
    failed = []

    for u in urls:
        try:
            p = crawl_one(u, out_dir, filename=filename)
            ok_paths.append(p)
        except Exception as e:
            failed.append((u, str(e)))

    for p in ok_paths:
        print(p)

    if failed:
        for u, err in failed:
            print(f"FAILED {u}: {err}", file=sys.stderr)
        return 1

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
